{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for f in os.listdir(os.path.join(\"data\",\"train\")):\n",
    "    path = os.path.join(\"data\",\"train\",f)\n",
    "    if os.path.isfile(path):\n",
    "        labels.append(path)\n",
    "\n",
    "train_doc_vecs = []\n",
    "train_Y = []\n",
    "for label_file in labels:\n",
    "    sentences = open(label_file, \"r\").readlines()\n",
    "    for sentence in sentences:\n",
    "        if len(sentence.lstrip().rstrip()) > 0:\n",
    "            vec_seq = []\n",
    "            for token in nlp(unicode(sentence)):\n",
    "                vec_seq.append(token.vector)\n",
    "            train_vec_seqs.append(np.array(vec_seq))\n",
    "            train_Y.append(int(label_file.split(\"label_\")[1].split(\".txt\")[0]))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make sequences equal length\n",
    "maxlen = 40\n",
    "new_seqs = []\n",
    "for vec_seq in train_vec_seqs:\n",
    "    orig_len, vec_len = np.shape(vec_seq)\n",
    "    if orig_len < maxlen:\n",
    "        new = np.zeros((maxlen,vec_len))\n",
    "        new[maxlen-orig_len:,:] = vec_seq\n",
    "    else:\n",
    "        new = sequence[orig_len-maxlen:,:]\n",
    "    new_seqs.append(new)\n",
    "    \n",
    "train_X = new_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X = train_vec_seqs\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_X, train_Y, test_size=0.1)\n",
    "\n",
    "y_train, y_test = [np_utils.to_categorical(x,nb_classes=4) for x in (y_train,y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, GRU, LSTM,  Input, merge\n",
    "\n",
    "ip = Input(shape=(maxlen,300), dtype='float32')\n",
    "normal = LSTM(100,dropout_W=0.1,dropout_U=0.1)(ip)\n",
    "reverse = LSTM(100,dropout_W=0.1,dropout_U=0.1,go_backwards=True)(ip)\n",
    "merged = merge([normal, reverse], mode='concat', concat_axis=-1)\n",
    "dropout = Dropout(0.1)(merged)\n",
    "output = Dense(4, activation='softmax')(dropout)\n",
    "model = Model(input=ip, output=output)\n",
    "model.compile('adam','categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(np.array(x_train), y_train, batch_size=20,nb_epoch = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(np.array(x_test), y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "\n",
    "model_json = model.to_json()\n",
    "jf = open(\"model.json\",\"w\")\n",
    "jf.write(model_json)\n",
    "model.save_weights(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_input(input_str):\n",
    "    doc = nlp(unicode(input_str))\n",
    "    vec_seq = []\n",
    "    for token in doc:\n",
    "        vec_seq.append(token.vector)\n",
    "    orig_len, vec_len = np.shape(vec_seq)\n",
    "    new = []\n",
    "    if orig_len < maxlen:\n",
    "        new = np.zeros((maxlen,vec_len))\n",
    "        new[maxlen-orig_len:,:] = vec_seq\n",
    "    else:\n",
    "        new = vec_seq[orig_len-maxlen:,:]\n",
    "    return np.array([new])\n",
    "\n",
    "model.predict(np.array(transform_input(\"enlighten me about the computer stats\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "debug = True\n",
    "def load_model(mpath=\"\",wpath=\"\"):\n",
    "\tif mpath==\"\" and wpath==\"\":\n",
    "\t\tmpath = \"data/model.json\"\n",
    "\t\twpath = \"data/model.h5\"\n",
    "\t# load json and create model\n",
    "\tjson_file = open(mpath, 'r')\n",
    "\tmodel_json = json_file.read()\n",
    "\tjson_file.close()\n",
    "\tmodel = model_from_json(model_json)\t\n",
    "\t# load weights into new model\n",
    "\tmodel.load_weights(wpath)\n",
    "\n",
    "\tif debug:\n",
    "\t\tprint \"Model loaded. Compiling...\"\n",
    "\tmodel.compile('adam','categorical_crossentropy')\n",
    "\n",
    "\tif debug:\n",
    "\t\tprint \"Model compiled.\"\n",
    "\t\tprint model.summary()\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import plot\n",
    "plot(r, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "def transform_input(input_str):\n",
    "    doc = nlp(unicode(input_str))\n",
    "    vec_seq = []\n",
    "    for token in doc:\n",
    "        vec_seq.append(token.vector)\n",
    "    orig_len, vec_len = np.shape(vec_seq)\n",
    "    new = []\n",
    "    if orig_len < maxlen:\n",
    "        new = np.zeros((maxlen,vec_len))\n",
    "        new[maxlen-orig_len:,:] = vec_seq\n",
    "    else:\n",
    "        new = vec_seq[orig_len-maxlen:,:]\n",
    "    return np.array([new])\n",
    "\n",
    "r.predict(np.array(transform_input(\"enlighten me about the computer stats\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
